{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7544407c-d766-4352-b347-476432dc4a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import torch\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cabd9ef-e142-44a5-87fb-373a3aab0d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class ICAE(torch.nn.Module):\n",
    "    def __init__(self, model_args, training_args, lora_config):\n",
    "        super().__init__()\n",
    "        self.model_args = model_args\n",
    "        self.training_args = training_args\n",
    "        self.model_name = model_args.model_name_or_path\n",
    "        self.icae = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=torch.float16 if training_args.bf16 is False else torch.bfloat16, use_flash_attention_2=True, resume_download=True)\n",
    "        \n",
    "        self.training = self.model_args.train    \n",
    "        \n",
    "        if self.training:    # indepedent model for gradient checkpointing\n",
    "            self.decoder = AutoModelForCausalLM.from_pretrained(self.model_name, torch_dtype=torch.float16 if training_args.bf16 is False else torch.bfloat16, use_flash_attention_2=True, resume_download=True)\n",
    "\n",
    "        self.vocab_size = self.icae.config.vocab_size + 1    # [PAD] token\n",
    "        self.pad_token_id = self.vocab_size - 1\n",
    "        self.mean_compression_rate = training_args.mean_compression_rate\n",
    "\n",
    "        # tunable\n",
    "        self.mem_size = self.training_args.fixed_mem_size\n",
    "        self.vocab_size_with_mem = self.vocab_size + self.mem_size # so, the mem tokens are in the range [self.vocab_size, self.vocab_size + self.mem_size)\n",
    "\n",
    "        # special tokens in addition to mem and length tokens\n",
    "        self.ae_token_id = self.vocab_size_with_mem + 0\n",
    "        self.lm_token_id = self.vocab_size_with_mem + 1\n",
    "        self.ft_token_id = self.vocab_size_with_mem + 2        \n",
    "\n",
    "        self.icae.resize_token_embeddings(self.vocab_size_with_mem + 3) \n",
    "        \n",
    "        # special tokens for Llama-2/Mistral tokenizer\n",
    "        self.bos_id = 1\n",
    "        self.eos_id = 2\n",
    "        \n",
    "        self.dim = self.icae.config.hidden_size\n",
    "        self.icae = get_peft_model(self.icae, lora_config)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.memory_token_embed = nn.Embedding(self.mem_size + 3, self.dim, padding_idx=None)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, use_fast=False)\n",
    "        self.append_sequence = torch.arange(self.vocab_size, self.vocab_size + self.mem_size, dtype=torch.long, device=device).unsqueeze(0)    # mem tokens\n",
    "        \n",
    "        if self.training:\n",
    "            self.init()\n",
    "\n",
    "\n",
    "    def init(self):\n",
    "        print(\"Freezing the decoder...\")\n",
    "        freeze_model(self.decoder)\n",
    "        self.decoder.eval()\n",
    "        print_trainable_parameters(self)\n",
    "        if self.training_args.restore_from is not None and self.training_args.restore_from != \"\":\n",
    "            print(f\"Loading from the pretrained checkpoint: {self.training_args.restore_from}...\")\n",
    "            state_dict = load_file(self.training_args.restore_from)\n",
    "            self.load_state_dict(state_dict)\n",
    "            print(f\"Finished loading from {self.training_args.restore_from}\")\n",
    "        print(\"Enabling gradient checkpointing...\")\n",
    "        # self.icae.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "        self.decoder.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "                \n",
    "        \n",
    "    def compute_num_segments(self, total_length):\n",
    "        assert total_length > 0\n",
    "        num_segments = math.ceil(total_length / (self.mem_size * self.mean_compression_rate))\n",
    "        return num_segments\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        prompt_answer_ids: torch.LongTensor = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "    ):\n",
    "        # encoder part\n",
    "        batch_size = input_ids.size(0)\n",
    "        total_length = input_ids.size(1)\n",
    "        num_segments = self.compute_num_segments(total_length)\n",
    "        segment_length = math.ceil(total_length / num_segments)\n",
    "        \n",
    "        prompt_answer_embs = self.icae.get_base_model().model.embed_tokens(prompt_answer_ids)\n",
    "        max_compressed_length = num_segments * self.mem_size\n",
    "        compress_outputs = torch.zeros((max_compressed_length, self.dim)).to(prompt_answer_embs)\n",
    "        \n",
    "        for segment_idx in range(num_segments):\n",
    "            \n",
    "            start_idx = segment_idx * segment_length\n",
    "            end_idx = min((segment_idx + 1) * segment_length, total_length)\n",
    "            segment_input_ids = input_ids[:, start_idx:end_idx]\n",
    "            segment_input_ids = torch.cat([segment_input_ids, self.append_sequence], dim=1)\n",
    "            mem_flag = segment_input_ids >= self.vocab_size\n",
    "\n",
    "            segment_input_embedding = self.icae.get_base_model().model.embed_tokens(segment_input_ids)\n",
    "            segment_input_embedding[mem_flag] = self.memory_token_embed(segment_input_ids[mem_flag] - self.vocab_size).to(segment_input_embedding)\n",
    "\n",
    "            # compress the current segment\n",
    "            segment_compress_outputs = self.icae(inputs_embeds=segment_input_embedding, output_hidden_states=True)\n",
    "            segment_compress_outputs = segment_compress_outputs.hidden_states[-1]\n",
    "\n",
    "            # collect memory tokens\n",
    "            compress_outputs[segment_idx*self.mem_size: self.mem_size*(segment_idx+1)] = segment_compress_outputs[mem_flag]\n",
    "            \n",
    "            del segment_input_ids, segment_input_embedding\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # decoder part\n",
    "        decoder_mem_flag = (prompt_answer_ids >= self.vocab_size) & (prompt_answer_ids < self.vocab_size + self.mem_size)   # only mem tokens\n",
    "\n",
    "        prompt_answer_embs[decoder_mem_flag] = compress_outputs  # replace memory slots\n",
    "        special_prompt = prompt_answer_ids >= self.vocab_size_with_mem\n",
    "        prompt_answer_embs[special_prompt] = self.memory_token_embed(prompt_answer_ids[special_prompt] - self.vocab_size).to(prompt_answer_embs)    # replace special token's embedding from self.memory_token_embed\n",
    "        \n",
    "        if self.training:   # has an independent se.f.decoder\n",
    "            decoder_outputs = self.decoder(inputs_embeds=prompt_answer_embs, output_hidden_states=True)\n",
    "        else:\n",
    "            with self.icae.disable_adapter():   # no independent decoder; use self.icae\n",
    "                decoder_outputs = self.icae(inputs_embeds=prompt_answer_embs, output_hidden_states=True)\n",
    "\n",
    "\n",
    "        logits = decoder_outputs.logits\n",
    "        effective_logits = logits[:,:-1,:].reshape(-1, logits.size(-1))\n",
    "        target_ids = labels[:,1:].reshape(-1)\n",
    "        loss = self.loss_fct(effective_logits, target_ids)\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "    \n",
    "    \n",
    "    def tokens_to_embeddings(self, token_ids):   # input_tokens can be either normal tokens and special tokens\n",
    "        embeddings = self.icae.get_base_model().model.embed_tokens(token_ids)\n",
    "        special_flags = token_ids >= self.vocab_size\n",
    "        embeddings[special_flags] = self.memory_token_embed(token_ids[special_flags] - self.vocab_size).to(embeddings)    # replace special token's embedding from self.memory_token_embed\n",
    "        return embeddings\n",
    "        \n",
    "    \n",
    "    def _compress(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None\n",
    "    ):  # for inference; compress a fixed length of input into memory slots\n",
    "\n",
    "        batch_size = input_ids.size(0)\n",
    "        total_length = input_ids.size(1)\n",
    "        num_segments = self.compute_num_segments(total_length)\n",
    "        segment_length = math.ceil(total_length / num_segments)\n",
    "        \n",
    "        max_compressed_length = num_segments * self.mem_size\n",
    "        compress_outputs = torch.zeros((max_compressed_length, self.dim))\n",
    "        \n",
    "        for segment_idx in range(num_segments):\n",
    "            start_idx = segment_idx * segment_length\n",
    "            end_idx = min((segment_idx + 1) * segment_length, total_length)\n",
    "            segment_input_ids = input_ids[:, start_idx:end_idx]\n",
    "            segment_input_ids = torch.cat([segment_input_ids, self.append_sequence], dim=1)\n",
    "            mem_flag = segment_input_ids >= self.vocab_size\n",
    "\n",
    "            segment_input_embedding = self.icae.get_base_model().model.embed_tokens(segment_input_ids)\n",
    "            segment_input_embedding[mem_flag] = self.memory_token_embed(segment_input_ids[mem_flag] - self.vocab_size).to(segment_input_embedding)\n",
    "\n",
    "            # compress the current segment\n",
    "            segment_compress_outputs = self.icae(inputs_embeds=segment_input_embedding, output_hidden_states=True)\n",
    "            segment_compress_outputs = segment_compress_outputs.hidden_states[-1]\n",
    "\n",
    "            # collect memory tokens\n",
    "            compress_outputs[segment_idx*self.mem_size: self.mem_size*(segment_idx+1)] = segment_compress_outputs[mem_flag]\n",
    "            \n",
    "            del segment_input_ids, segment_input_embedding\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return compress_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41afaaef-7357-4c90-874c-932e77efc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from dataclasses import dataclass, field\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: str = field(default=\"mistralai/Mistral-7B-v0.1\")\n",
    "    lora_r: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"lora rank\"}\n",
    "    )\n",
    "    lora_dropout: float = field(\n",
    "        default=0.05,\n",
    "        metadata={\"help\": \"lora dropout\"}\n",
    "    )\n",
    "    train: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"if true, the model ckpt will be initialized for training; else, it's for inference\"}\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\n",
    "    debug_data: bool = field(default=False, metadata={\"help\": \"Enable debug dataset to quickly verify the training process\"})\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=28000,\n",
    "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )\n",
    "    fixed_mem_size: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"Enalbing the fixed mem size.\"},\n",
    "    )\n",
    "    mean_compression_rate: int = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Mean compression rate; default=4\"},\n",
    "    )\n",
    "    min_tokens_for_lm: int = field(\n",
    "        default=64,\n",
    "        metadata={\"help\": \"Minimum tokens for lm objective learning\"},\n",
    "    )\n",
    "    leave_tokens_for_lm: int = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"Leave some tokens without loss for lm objective\"},\n",
    "    )\n",
    "    lm_ratio: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\"help\": \"Ratio for LM training.\"},\n",
    "    )\n",
    "    add_special_token_for_lm: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add a special token for the prompt of language modeling; default: False\"},\n",
    "    )\n",
    "    restore_from: str = field(\n",
    "        default=\"\",\n",
    "        metadata={\"help\": \"The checkpoint that should be restored from for fine-tuning\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfbea413-9536-4cad-b4be-fd11219b9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments()\n",
    "data_args = DataArguments()\n",
    "training_args = TrainingArguments(output_dir=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4fdc87a-6059-4ece-b65d-23ba2648cf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArguments(model_name_or_path='mistralai/Mistral-7B-v0.1', lora_r=128, lora_dropout=0.05, train=True)\n"
     ]
    }
   ],
   "source": [
    "print(model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc38ac59-e79c-4ca9-b473-4b9c604f6d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.gradient_checkpointing_kwargs = {\"use_reentrant\": False}  # manually add this argument in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6013e1a-364b-4d38-b15b-752dbb905ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    LoraConfig,\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "        r=model_args.lora_r,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9095392-fd1e-4ef2-afeb-a3d23c1ccd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (training_args.fixed_mem_size & (training_args.fixed_mem_size - 1)) == 0, \"training_args.fixed_mem_size must be a power of 2\"    \n",
    "assert training_args.leave_tokens_for_lm <= training_args.min_tokens_for_lm, \"leave_tokens_for_lm should be fewer than min_tokens_for_lm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb7578d1-624a-4a52-8903-24936e8aeebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_size = training_args.fixed_mem_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7bc4f8d-cb2b-477d-9296-32a4596fdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/path/to/train/file\"\n",
    "eval_file = \"/path/to/dev/file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71b8da26-29ec-4ad8-9ea5-c7862446ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m: eval_file}, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# streaming can be removed if the dataset is not very large.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": train_file, \"eval\": eval_file}, streaming=True) # streaming can be removed if the dataset is not very large.\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"eval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479d432-4aa7-40ee-8813-ec2955a0624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorForDynamicPadding:\n",
    "    def __init__(self, pad_token_id, pad_to_multiple_of=None):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "    def __call__(self, examples):\n",
    "        input_ids = [torch.tensor(example[\"input_ids\"], dtype=torch.long) for example in examples]\n",
    "        labels = [torch.tensor(example[\"labels\"], dtype=torch.long) for example in examples]\n",
    "        prompt_answer_ids = [torch.tensor(example[\"prompt_answer_ids\"], dtype=torch.long) for example in examples]\n",
    "        input_ids = self.dynamic_padding(input_ids, fill_value=self.pad_token_id)\n",
    "        prompt_answer_ids = self.dynamic_padding(prompt_answer_ids, fill_value=self.pad_token_id)\n",
    "        labels = self.dynamic_padding(labels)\n",
    "        batch = {\"input_ids\": input_ids, \"labels\": labels, \"prompt_answer_ids\": prompt_answer_ids}\n",
    "        return batch\n",
    "    def dynamic_padding(self, sequences, fill_value=-100):\n",
    "        max_length = max(len(x) for x in sequences)\n",
    "        if self.pad_to_multiple_of:\n",
    "            max_length = ((max_length - 1) // self.pad_to_multiple_of + 1) * self.pad_to_multiple_of\n",
    "        padded_sequences = torch.full((len(sequences), max_length), fill_value, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            padded_sequences[i, :len(seq)] = seq\n",
    "        return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733219fa-f05d-4569-b2bf-a4c4dab89961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_tokenize_function(examples, model, mem, lm_ratio=0.0):\n",
    "    text_output = model.tokenizer(examples[\"text\"], truncation=False, padding=False, return_attention_mask=False)\n",
    "    text_output['prompt_answer_ids'] = []\n",
    "    text_output['labels'] = []\n",
    "\n",
    "    max_len = model.training_args.model_max_length  # heuristic\n",
    "\n",
    "    for idx in range(len(text_output[\"input_ids\"])):\n",
    "        \n",
    "        ae = True\n",
    "        a, b = text_extraction(text_output[\"input_ids\"][idx], max_len, lm_ratio=lm_ratio)\n",
    "        length_a = len(a)\n",
    "        num_segments = model.compute_num_segments(length_a)\n",
    "        total_mem_length = num_segments * model.mem_size\n",
    "        \n",
    "        if len(b) > model.training_args.min_tokens_for_lm:  # avoid too few tokens for lm, which is a waste of computing\n",
    "            ae = False\n",
    "            b = b[:max_len]\n",
    "\n",
    "        text_output['input_ids'][idx] = a\n",
    "\n",
    "        # decoder part: note that in v2, we add mem_tokens to the prompt_ids for easy implementation; which is different from v1 implementation where mem tokens are not in the prompt_ids\n",
    "        if ae:  # autoencoding objective\n",
    "            prompt_ids = [mem[0]] * total_mem_length + [model.ae_token_id]\n",
    "            answer_ids = a + [model.eos_id]    # if ae, eos token\n",
    "        else:   # lm objective\n",
    "            prompt_ids = [mem[0]] * total_mem_length\n",
    "            if model.training_args.add_special_token_for_lm:\n",
    "                prompt_ids += [model.lm_token_id]\n",
    "            answer_ids = b   # if lm, no eos token\n",
    "\n",
    "        text_output['prompt_answer_ids'].append(prompt_ids + answer_ids)\n",
    "        if ae:\n",
    "            labels = [-100] * len(prompt_ids) + answer_ids\n",
    "        else:\n",
    "            labels = [-100] * len(prompt_ids) + [-100] * model.training_args.leave_tokens_for_lm + answer_ids[model.training_args.leave_tokens_for_lm:] # no loss for leave_tokens_for_lm\n",
    "        text_output['labels'].append(labels)\n",
    "        assert len(text_output['prompt_answer_ids'][-1]) == len(labels)\n",
    "        \n",
    "    return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f953430-b3c6-4246-a16d-aad9bb039557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "def train_model(model, train_dataset, eval_dataset, training_args, data_collator=None):\n",
    "\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            print(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "    \n",
    "    if max(training_args.per_device_train_batch_size, training_args.per_device_eval_batch_size) == 1:\n",
    "        data_collator = None\n",
    "        \n",
    "    # print training_args at local_rank 0\n",
    "    local_rank = int(os.getenv('LOCAL_RANK', '0'))\n",
    "    if local_rank == 0:\n",
    "        print(training_args)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    checkpoint = None\n",
    "    \n",
    "    if training_args.resume_from_checkpoint is not None:\n",
    "        checkpoint = training_args.resume_from_checkpoint\n",
    "    elif last_checkpoint is not None:\n",
    "        checkpoint = last_checkpoint\n",
    "\n",
    "    print(f\"Loaded from the checkpoint: {checkpoint}\")\n",
    "\n",
    "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model()\n",
    "    trainer.log_metrics(\"train\", train_result.metrics)\n",
    "    metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7e14d2-a318-43fe-84c4-d29901970e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ICAE(model_args, training_args, lora_config)\n",
    "MEM_TOKENS = list(range(model.vocab_size, model.vocab_size + memory_size))\n",
    "\n",
    "train_dataset = train_dataset.map(pretrain_tokenize_function, batched=True, batch_size=64, fn_kwargs={\"model\": model, \"mem\": MEM_TOKENS, \"lm_ratio\": training_args.lm_ratio})\n",
    "eval_dataset = eval_dataset.map(pretrain_tokenize_function, batched=True, fn_kwargs={\"model\": model, \"mem\": MEM_TOKENS})   # don't add lm in the dev set.\n",
    "\n",
    "data_collator = DataCollatorForDynamicPadding(model.pad_token_id)\n",
    "train_model(model, train_dataset, eval_dataset, training_args, data_collator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
